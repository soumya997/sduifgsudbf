<p><strong>Hi</strong> reader, this is a small and simple guide to RNNs, we will discuss all the basic requirements that you need to get started with RNNs from underneath concepts to code implementation. We will be implementing using TensorFlow 2.0.</p>
<h1 id="indepth-rnn-and-implementation-using-tf2.x">Indepth RNN and implementation using tf2.x</h1>
<p><img src="https://i.imgur.com/KXbnThQ.png" alt="RNN struc" /> <br></p>
<p><strong>Hi</strong> reader, this is a small and simple guide to RNNs, we will discuss all the basic requirements that you need to get started with RNNs from underneath concepts to code implementation. We will be implementing using TensorFlow 2.0.</p>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script type="text/javascript" src="https://github.com/kashif-umair/gist-embed/raw/master/gist-embed.js"></script>

<p><code id="gist-4672365" data-file="2.java" data-line="2-5,10-14,11,20"></code></p>
<p><br> # Table of Content: 1. <strong>What is RNNs</strong> 2. <strong>Many to One RNN</strong> 3. <strong>Some thing about the data</strong> 4. <strong>Loading Data Using Pandas</strong> 5. <strong>Some Basic Exploratory Data Analysis</strong> 6. <strong>Data Preprocessing</strong> 7. <strong>Model Building</strong> 8. <strong>Train Test Split</strong> 9. <strong>Model Training</strong> 10. <strong>Model Evaluation</strong> 11. <strong>Conclusion</strong> <br></p>
<h3 id="what-is-rnns">What is RNNs?</h3>
<p>The R in RNN stands for recurrent which literally means repeating. Now you will think what is repeating here? Here repeating refers to the repeating structure of RNNs. There are different kinds of data available, like normal tabular data with different features, image data,time-series data, text data, etc. From the time series, data and text data are the data which has a sequence in it. Let us discuss the meaning of sequence for text data. It's very noticeable that every sentence has a sequence of words, and if you change the sequence of that sentence then the sentence won't have the same meaning. Because of this reason, we can't use MLPs on text data (there are other reasons too like large parameter size,size of i/p word and o/p word can be different, etc.). Now, an RNN has many structures as per the problem requirements like- one to one, one to many, many to one, many to many as fig 1.</p>
<img src="https://i.imgur.com/AtbnrWR.jpg" alt="RNN structure" />
<h3>
Fig: 1
</h3>
<p><br> But, here I will be discussing many to one network. But, keep in mind that a single RNN cell is the same everywhere, just they are placed in different ways to satisfy the problem requirements.</p>
<h3 id="many-to-one-rnn">Many to One RNN:</h3>
<p>The structure of many to many looks like fig2.The application of many to one network is spam detection, sentiment analysis, etc. Now let's dive deep and understand the bare bones of RNN.</p>
<img src="https://i.imgur.com/Zz5bcjX.png" alt="Many to One" />
<h3>
Fig: 2
</h3>

<p>Every vertical block with input and output arrows is called a RNN cell. In the beginning, it uses a vector with zeros which are named here as <strong>O<sub>0</sub></strong>, this is only for showing the repeating structure of RNN. the circles in the blocks are activation functions. And if you check out carefully, you will observe that there are three types of weights.W which are with input word vector, <strong>W'</strong> is with RNN cell o/p and finally <strong>W''</strong> is at the final RNN cell which helps to make the prediction.O/p of RNN cells are taken as an i/p for the next RNN cell, this is to keep the sequence information preserved.</p>
<p>Now, <strong>O<sub>i</sub></strong> is,</p>
<p><strong>O<sub>i</sub> = f(X<sub>i</sub> W+O<sub>i</sub> W')</strong></p>
<p>And at the end, the output of the last RNN cell and the <strong>W''</strong> are taken and passed through an activation function(here sigmoid is taken) to generate the prediction, <strong>Y' = S(O<sub>n</sub> W'')</strong>. And rest of the things are taken care of by backpropagation and gradient descent stuff, normal model training. <br> &gt; Note, description of all symbols are given in the image itself.</p>
<h2 id="utils">Utils</h2>
<pre class="python3"><code>import warnings
warnings.filterwarnings(&quot;ignore&quot;)                     #Ignoring unnecessory warnings

import numpy as np                                  #for large and multi-dimensional arrays
import pandas as pd                                 #for data manipulation and analysis
import nltk                                         #Natural language processing tool-kit

from nltk.corpus import stopwords                   #Stopwords corpus
from nltk.stem import PorterStemmer                 # Stemmer

from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words
from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF
from gensim.models import Word2Vec                                   #For Word2Vec

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense

</code></pre>
<html>
  <head>
    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" ></script>
    <script type="text/javascript" src="https://github.com/kashif-umair/gist-embed/raw/feature/show_single_line/gist-embed.js" ></script>
  </head>
  <body>
    
<code id="gist-ca6b49b7c7181e916cb78e6d1e526da3" data-file="python_rgb_5.py" data-line="4"></code>
</body>
</html>


<h2 id="loading-data-using-pandas">Loading Data Using Pandas:</h2>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> pandas <span class="ch">as</span> pd
pd.pandas.set_option(<span class="st">&#39;display.max_columns&#39;</span>,<span class="ot">None</span>)
pd.pandas.set_option(<span class="st">&#39;display.max_rows&#39;</span>,<span class="ot">None</span>)
df = pd.read_csv(<span class="st">&#39;../input/amazon-fine-food-reviews/Reviews.csv&#39;</span>)</code></pre>
<h3 id="some-basic-exploratory-data-analysis">Some Basic Exploratory Data Analysis:</h3>
<pre class="sourceCode python"><code class="sourceCode python">df.info()</code></pre>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 568454 entries, 0 to 568453
Data columns (total 10 columns):
 #   Column                  Non-Null Count   Dtype 
---  ------                  --------------   ----- 
 0   Id                      568454 non-null  int64 
 1   ProductId               568454 non-null  object
 2   UserId                  568454 non-null  object
 3   ProfileName             568438 non-null  object
 4   HelpfulnessNumerator    568454 non-null  int64 
 5   HelpfulnessDenominator  568454 non-null  int64 
 6   Score                   568454 non-null  int64 
 7   Time                    568454 non-null  int64 
 8   Summary                 568427 non-null  object
 9   Text                    568454 non-null  object
dtypes: int64(5), object(5)
memory usage: 43.4+ MB</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">df.shape</code></pre>
<pre><code>(568454, 10)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">df.isna().<span class="dt">sum</span>()</code></pre>
<pre><code>Id                         0
ProductId                  0
UserId                     0
ProfileName               16
HelpfulnessNumerator       0
HelpfulnessDenominator     0
Score                      0
Time                       0
Summary                   27
Text                       0
dtype: int64</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">df1 = df[df[<span class="st">&#39;Score&#39;</span>]!=<span class="dv">3</span>]</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">df1.shape</code></pre>
<pre><code>(525814, 10)</code></pre>
<h3 id="data-preprocessing">Data Preprocessing:</h3>
<p>Here, we are dropping the data points where <code>UseId</code>,<code>&quot;ProfileName</code>,<code>Time</code>,<code>Text</code> are same and keeping the first data point.</p>
<pre class="sourceCode python"><code class="sourceCode python">df1 = df1.drop_duplicates(subset={<span class="st">&quot;UserId&quot;</span>,<span class="st">&quot;ProfileName&quot;</span>,<span class="st">&quot;Time&quot;</span>,<span class="st">&quot;Text&quot;</span>}, keep=<span class="st">&#39;first&#39;</span>, inplace=<span class="ot">False</span>)</code></pre>
<p>Here, we are keeping the values where <code>HelpfulnessNumerator</code> &lt;= <code>HelpfulnessDenominator</code>.</p>
<pre class="sourceCode python"><code class="sourceCode python">df1 = df1[df1[<span class="st">&#39;HelpfulnessNumerator&#39;</span>]&lt;=df1[<span class="st">&#39;HelpfulnessDenominator&#39;</span>]]</code></pre>
<p>As we have taken this dataset as a NLP problem, so we are only considering the text column as the i/p data and <code>score</code> as the labels.</p>
<pre class="sourceCode python"><code class="sourceCode python">list1 = <span class="dt">list</span>(df1[<span class="st">&#39;Score&#39;</span>])
list2 = <span class="dt">list</span>(df1[<span class="st">&#39;Text&#39;</span>])

score_df = pd.DataFrame(list1,columns = [<span class="st">&#39;score&#39;</span>])
text_df = pd.DataFrame(list2,columns = [<span class="st">&#39;text&#39;</span>])

df2 = pd.concat([text_df,score_df],axis=<span class="dv">1</span>)
df2.head()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">text</th>
<th align="left">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left">I have bought several of the Vitality canned d...</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">Product arrived labeled as Jumbo Salted Peanut...</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">This is a confection that has been around a fe...</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">If you are looking for the secret ingredient i...</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">Great taffy at a great price. There was a wid...</td>
<td align="left">5</td>
</tr>
</tbody>
</table>
<p>Checking out some of the text which starts from 500 index and ends at 550 index.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">500</span>,<span class="dv">505</span>):
    <span class="dt">print</span>(df2[<span class="st">&#39;text&#39;</span>].values[i])
    <span class="dt">print</span>(<span class="st">&#39;-----------------------------------------------------&#39;</span>)</code></pre>
<pre><code>...you can absolutely forget about these. Confirmed by other reviewers, these chips are now total garbage. Like chewing on styrofoam packaging &quot;peanuts&quot;. Positively awful, no hyperbole or exaggeration. I&#39;ll NEVER buy anything from Kettle brand ever again! From a reportedly once great &quot;premium&quot; brand, literally any mass market chip I&#39;ve ever tried tastes better than these. Stale and rancid tasting, and virtually no salty taste whatsoever. Completely awful!
-----------------------------------------------------
These chips are nasty.  I thought someone had spilled a drink in the bag, no the chips were just soaked with grease.  Nasty!!
-----------------------------------------------------
Unless you like salt vinegar chips as salty as eating actual pinches of salt and drinking actual vinegar, i doubt you will like these chips.  These are the saltiest &amp; sourest chips I have ever had, and the only reason stops me from throwing these away is because I paid for 2 full boxes and dont like to waste food.  The brown chips are especially bad, besides being salty and sour, they also taste overcooked and burnt.  Unless you are the rare kind that can take this kind of extreme taste, you will not like these chips.&lt;br /&gt;&lt;br /&gt;I actually have a high tolerance for sour taste, so I can down a bag of chips with a bit of difficulty.  But normal people, please do not try this at home.
-----------------------------------------------------
I bought this brand as a trial since I am tired of the Pingos.&lt;br /&gt;&lt;br /&gt;It claims that it is natural. I have no argument on this. But the point is that more than 50% in the bag is over-fried and in brown color. I really suffer eating the over-fried chips. I open some other bags and it looks like the same. So I just throw away all of them. I don&#39;t know if I was with bad luck or every bag they are selling is the same. But for sure I will never buy this brand any more.
-----------------------------------------------------
They are good but wish they were also baked. Have not found baked no salt potato chips anywhere. If there are any I wish someone would post.
-----------------------------------------------------</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">df3 = df2.head(<span class="dv">50000</span>)
df3.head()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">text</th>
<th align="left">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left">I have bought several of the Vitality canned d...</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">Product arrived labeled as Jumbo Salted Peanut...</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">This is a confection that has been around a fe...</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">If you are looking for the secret ingredient i...</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">Great taffy at a great price. There was a wid...</td>
<td align="left">5</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>Here we have created a function which will mark the score from 1 to 3 as negative and 4 to 5 as positive. A we are going to do binary classification that's why we are keeping the labels to positive and negative.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> partition(x):
    <span class="kw">if</span> x &lt; <span class="dv">3</span>:
        <span class="kw">return</span> <span class="st">&#39;negative&#39;</span>
    <span class="kw">return</span> <span class="st">&#39;positive&#39;</span>

score_upd = df3[<span class="st">&#39;score&#39;</span>]
t = score_upd.<span class="dt">map</span>(partition)
df3[<span class="st">&#39;score&#39;</span>]=t

df3.head()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">text</th>
<th align="left">score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left">I have bought several of the Vitality canned d...</td>
<td align="left">positive</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">Product arrived labeled as Jumbo Salted Peanut...</td>
<td align="left">negative</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">This is a confection that has been around a fe...</td>
<td align="left">positive</td>
</tr>
<tr class="even">
<td align="left">3</td>
<td align="left">If you are looking for the secret ingredient i...</td>
<td align="left">negative</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">Great taffy at a great price. There was a wid...</td>
<td align="left">positive</td>
</tr>
</tbody>
</table>
<pre class="sourceCode python"><code class="sourceCode python">df3.isna().<span class="dt">sum</span>()</code></pre>
<pre><code>text     0
score    0
dtype: int64</code></pre>
<p>Here, df_x is the i/p text data and df_y is the i/p lable.</p>
<pre class="sourceCode python"><code class="sourceCode python">df_x = df3[<span class="st">&#39;text&#39;</span>]
df_y = df3[<span class="st">&#39;score&#39;</span>]</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">stop_words = <span class="dt">set</span>(stopwords.words(<span class="st">&#39;english&#39;</span>))
<span class="dt">len</span>(stop_words) <span class="co">#finding stop words</span></code></pre>
<pre><code>179</code></pre>
<p>Here we are doing the real pre-processing,which are like- keeping only the alphabets,making all the alphabets lower,removing all the stop word.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> re
<span class="ch">from</span> nltk.corpus <span class="ch">import</span> stopwords
<span class="ch">from</span> nltk.stem.porter <span class="ch">import</span> PorterStemmer
snow = nltk.stem.SnowballStemmer(<span class="st">&#39;english&#39;</span>)

corpus = []
<span class="kw">for</span> i in <span class="dt">range</span>(<span class="dv">0</span>, <span class="dt">len</span>(df3)):
    review = re.sub(<span class="st">&#39;[^a-zA-Z]&#39;</span>, <span class="st">&#39; &#39;</span>, df3[<span class="st">&#39;text&#39;</span>][i])
    review = review.lower()
    review = review.split()
    
    review = [snow.stem(word) <span class="kw">for</span> word in review <span class="kw">if</span> not word in stopwords.words(<span class="st">&#39;english&#39;</span>)]
    review = <span class="st">&#39; &#39;</span>.join(review)
    corpus.append(review)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">corpus[<span class="dv">1</span>]</code></pre>
<pre><code>&#39;product arriv label jumbo salt peanut peanut actual small size unsalt sure error vendor intend repres product jumbo&#39;</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">df_x = corpus</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="dt">type</span>(df_x)</code></pre>
<pre><code>list</code></pre>
<p>This is an important step, here we are creating word vectors by doing one hot encoing, and we are only taking 5000 words as the dictionary.</p>
<pre class="sourceCode python"><code class="sourceCode python">voc_size=<span class="dv">5000</span>
onehot_repr=[one_hot(words,voc_size)<span class="kw">for</span> words in corpus] 
<span class="dt">type</span>(onehot_repr)</code></pre>
<pre><code>list</code></pre>
<p>Padding is one of the most important part before feeding the data to the model. In simple way padding is a way to keep the input size same for all i/p text by adding zeros at the front. Here we are considering each i/p text corpus will be of 400 words.</p>
<pre class="sourceCode python"><code class="sourceCode python">sent_length=<span class="dv">400</span>
embedded_docs=pad_sequences(onehot_repr,padding=<span class="st">&#39;pre&#39;</span>,maxlen=sent_length)
<span class="dt">print</span>(embedded_docs)</code></pre>
<pre><code>[[   0    0    0 ... 4896 3622 1277]
 [   0    0    0 ... 4842 3622 1244]
 [   0    0    0 ...  862 3777 4073]
 ...
 [   0    0    0 ... 2125 2180 4235]
 [   0    0    0 ... 1025 2246 1680]
 [   0    0    0 ...  744 4878  397]]</code></pre>
<p>This is the model build using tensorflow 2.0. Think about the whole model like this,</p>
<p>first its a sequencial model starting with a <code>Embedding</code> layer (wich is deep learning version of word to vec,or more intuitively WV is an example of <code>Embedding</code> and we are use it because we can be trained,it's not static like WV) and input size is 5000x40 and output size of 400x40.Now we created a dropout layer using <code>Dropout()</code> which will reduce the chance of overfitting. After that we are taking 100 layers of <code>RNN</code> and feeding the output of Embedding leyer to each of the <code>RNN</code> layer . Now, each of the <code>RNN</code> layer will spit out a scaller value and then we will be stacking them out,because of that the output size after <code>RNN</code> layer would be 100. Ten we againg perform a Dropout layer. At the end we have a single activation unit of <code>sigmoid</code> (line 8) which will take the 100 sized vector and give a prediction or in general words give a scaler value between 0 to 1.</p>
<p>Now we are setting the optimixer as <code>adam</code> and evaluation matrics as <code>accurecy</code> and loss as <code>binary_crossentropy</code>(this is because we have taken this as a binary classification problem) using the <code>model.compile()</code> method. At the 10th line we are printing the model architecture using <code>model.summary()</code> method.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co">## Creating model</span>
embedding_vector_features=<span class="dv">40</span>
model=Sequential()
model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))
model.add(Dropout(<span class="fl">0.3</span>))
model.add(SimpleRNN(<span class="dv">100</span>))
model.add(Dropout(<span class="fl">0.3</span>))
model.add(Dense(<span class="dv">1</span>,activation=<span class="st">&#39;sigmoid&#39;</span>))
model.<span class="dt">compile</span>(loss=<span class="st">&#39;binary_crossentropy&#39;</span>,optimizer=<span class="st">&#39;adam&#39;</span>,metrics=[<span class="st">&#39;accuracy&#39;</span>])
<span class="dt">print</span>(model.summary())</code></pre>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 400, 40)           200000    
_________________________________________________________________
dropout_1 (Dropout)          (None, 400, 40)           0         
_________________________________________________________________
simple_rnn (SimpleRNN)       (None, 100)               14100     
_________________________________________________________________
dropout_2 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 101       
=================================================================
Total params: 214,201
Trainable params: 214,201
Non-trainable params: 0
_________________________________________________________________
None</code></pre>
<p>Some preprocessing before feeding the data. We are doing lable encoding of the <code>score</code> column.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> sklearn.preprocessing <span class="ch">import</span> LabelEncoder
encode = LabelEncoder()
df_y2 = encode.fit_transform(df_y)
<span class="dt">type</span>(df_y2)</code></pre>
<pre><code>numpy.ndarray</code></pre>
<p>This is a important part, where we car converting our data to nd arrays as we cant just input a pandas data frame.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> numpy <span class="ch">as</span> np
X_final=np.array(embedded_docs)
y_final=np.array(df_y2)</code></pre>
<h3 id="train-test-split">Train Test Split:</h3>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> sklearn.model_selection <span class="ch">import</span> train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_final, y_final, test_size=<span class="fl">0.2</span>, random_state=<span class="dv">42</span>)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co">#we are feeding the </span>
model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=<span class="dv">10</span>,batch_size=<span class="dv">64</span>)</code></pre>
<pre><code>Epoch 1/10
625/625 [==============================] - 78s 125ms/step - loss: 0.3643 - accuracy: 0.8591 - val_loss: 0.2562 - val_accuracy: 0.8989
Epoch 2/10
625/625 [==============================] - 76s 121ms/step - loss: 0.3047 - accuracy: 0.8830 - val_loss: 0.3438 - val_accuracy: 0.8626
Epoch 3/10
625/625 [==============================] - 77s 124ms/step - loss: 0.2992 - accuracy: 0.8847 - val_loss: 0.2755 - val_accuracy: 0.8908
Epoch 4/10
625/625 [==============================] - 77s 124ms/step - loss: 0.2555 - accuracy: 0.9012 - val_loss: 0.2714 - val_accuracy: 0.8908
Epoch 5/10
625/625 [==============================] - 77s 123ms/step - loss: 0.2417 - accuracy: 0.9074 - val_loss: 0.2741 - val_accuracy: 0.8917
Epoch 6/10
625/625 [==============================] - 77s 123ms/step - loss: 0.2318 - accuracy: 0.9126 - val_loss: 0.2726 - val_accuracy: 0.8890
Epoch 7/10
625/625 [==============================] - 76s 122ms/step - loss: 0.2177 - accuracy: 0.9166 - val_loss: 0.2600 - val_accuracy: 0.8955
Epoch 8/10
625/625 [==============================] - 77s 123ms/step - loss: 0.2857 - accuracy: 0.8852 - val_loss: 0.2866 - val_accuracy: 0.8880
Epoch 9/10
625/625 [==============================] - 77s 124ms/step - loss: 0.3006 - accuracy: 0.8788 - val_loss: 0.3423 - val_accuracy: 0.8598
Epoch 10/10
625/625 [==============================] - 76s 122ms/step - loss: 0.2995 - accuracy: 0.8789 - val_loss: 0.2991 - val_accuracy: 0.8777





&lt;tensorflow.python.keras.callbacks.History at 0x7f3a10485050&gt;</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">y_pred_rnn = model.predict_classes(X_test)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> sklearn.metrics <span class="ch">import</span> accuracy_score
accuracy_score(Y_test,y_pred_rnn)</code></pre>
<pre><code>0.8777</code></pre>
<h3 id="conclusion">Conclusion:</h3>
<p>Our model is giving nera 90% accurecy which is very good. This is how you train a RNN using tensorflow 2.0</p>
<p>Thank you for reading.</p>
